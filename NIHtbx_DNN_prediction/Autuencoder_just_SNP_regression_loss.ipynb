{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phenotype Predicting with SNP\n",
    "- In this code, target phenotype is NIH_totcomp\n",
    "- If you want to see a binary output(or classification), you have to modify \"if not use_rf:\" part\n",
    "\n",
    "#### [Info]\n",
    "- epoch:100\n",
    "- m = 64\n",
    "- classify_middle = [16]\n",
    "- all_layers = [768, 512, 256]\n",
    "- plotted MSE loss decrement\n",
    "- loss and r_squared score are saved in loss_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 잘 돌아가면 combine 1:4로 나눠서 train/test 따로 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============== input files ===============\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "combine = pd.read_csv('/home/ubuntu/gps_and_phe_nihtotcomp_notnull.csv', header=0)\n",
    "combine = combine.set_index('KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_result = pd.DataFrame(columns=['epoch', 'reconstuct loss', 'regression loss', 'r squared'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== constants ===============\n",
    "k_fold = 10\n",
    "num_samples = len(combine)\n",
    "in_channel = 524503-6 \n",
    "each_fold_size = int(np.ceil(num_samples/k_fold))\n",
    "\n",
    "# the following are used to normalize data\n",
    "max_abcd = 3.0\n",
    "max_label = 5.0\n",
    "min_label = -5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== parameters ===============\n",
    "#all_layers = [10000, 5000, 1000] # layers in ae, can also be changed to a command line parameter\n",
    "m = 64\n",
    "classify_middle = [16]\n",
    "all_layers = [768, 512, 256]\n",
    "\n",
    "num_epochs = 100\n",
    "batch = 50\n",
    "lr = 0.01\n",
    "debug = False\n",
    "binary_output = False\n",
    "use_rf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import h5py\n",
    "import gc\n",
    "from itertools import islice\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "import itertools\n",
    "import argparse\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== define network ===============\n",
    "class nn1(nn.Module):\n",
    "    def __init__(self, orig_middles, classify_middles, m, in_channel, y_dim):\n",
    "        super(nn1, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "\n",
    "\n",
    "        layers = []\n",
    "        middles = [in_channel] + orig_middles\n",
    "        self.f0 = nn.Linear(int(in_channel/2), middles[1]).cuda(0)\n",
    "        self.f1 = nn.Linear(in_channel - int(in_channel/2), middles[1]).cuda(1)\n",
    "        self.bn = nn.BatchNorm1d(num_features=middles[1]).cuda()\n",
    "        for i in range(1, len(middles)-1):\n",
    "            layers.append(nn.Linear(middles[i], middles[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(num_features=middles[i+1]))\n",
    "            layers.append(nn.ReLU(True))\n",
    "        layers.append(nn.Linear(middles[-1], m))\n",
    "        layers.append(nn.BatchNorm1d(num_features=m))\n",
    "        self.encoder = nn.Sequential(*layers).cuda()\n",
    "\n",
    "        layers2 = []\n",
    "        middles = [m] + orig_middles[::-1]\n",
    "        for i in range(len(middles)-1):\n",
    "            layers2.append(nn.Linear(middles[i], middles[i+1]))\n",
    "            layers2.append(nn.BatchNorm1d(num_features=middles[i+1]))\n",
    "            layers2.append(nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(*layers2).cuda()\n",
    "        self.e0 = nn.Linear(orig_middles[0], int(in_channel/2)).cuda(0)\n",
    "        self.e1 = nn.Linear(orig_middles[0], in_channel - int(in_channel/2)).cuda(1)\n",
    "        \n",
    "        layers3 = [] # regression/classification\n",
    "        middles2 = [m] + classify_middles\n",
    "        for i in range(len(middles2)-1):\n",
    "            layers3.append(nn.Linear(middles2[i], middles2[i+1]))\n",
    "            layers3.append(nn.BatchNorm1d(num_features=middles2[i+1]))\n",
    "            layers3.append(nn.ReLU(True))\n",
    "\n",
    "        if binary_output:\n",
    "            layers3.append(nn.Linear(middles2[-1], 2)) # binary classification\n",
    "            layers3.append(nn.BatchNorm1d(num_features=2))\n",
    "        else:\n",
    "            layers3.append(nn.Linear(middles2[-1], y_dim))\n",
    "            layers3.append(nn.BatchNorm1d(num_features=y_dim))\n",
    "        self.regression = nn.Sequential(*layers3).cuda()\n",
    "\n",
    "    def encode_and_predict(self, x):\n",
    "        new_x = self.f0(Variable(x[:, :int(self.in_channel/2)]).cuda(0))\n",
    "        new_x += self.f1(Variable(x[:, int(self.in_channel/2):]).cuda(1)).to('cuda:0')\n",
    "        #new_x = nn.Dropout(p=0.95)(new_x)\n",
    "        #new_x = self.bn(new_x)\n",
    "        new_x = nn.ReLU(True)(new_x)\n",
    "\n",
    "        encode_x = self.encoder(new_x)\n",
    "        predict_y = self.regression(encode_x)\n",
    "        return predict_y\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_x = self.f0(Variable(x[:, :int(self.in_channel/2)]).cuda(0))\n",
    "        new_x += self.f1(Variable(x[:, int(self.in_channel/2):]).cuda(1)).to('cuda:0')\n",
    "        new_x = nn.Dropout(p=0.95)(new_x)\n",
    "        new_x = self.bn(new_x)\n",
    "        new_x = nn.ReLU(True)(new_x)\n",
    "        \n",
    "        encode_x = self.encoder(new_x)\n",
    "        labels = self.regression(encode_x)\n",
    "        decode_x = self.decoder(encode_x)\n",
    "\n",
    "        return (nn.Sigmoid()(self.e0(decode_x)), nn.Sigmoid()(self.e1(decode_x.cuda(1)))), labels\n",
    "        #gpu(0)에서 계산한것 -> sigmoid ,  gpu(1)에서 계산한 것 -> sigmoid 이 두개를 순서쌍으로 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(learning_rate, m, batch, train_folds):\n",
    "    print('-'*50)\n",
    "    print('Training........')\n",
    "    \n",
    "    model = nn1(all_layers, classify_middle, m, in_channel, y_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch: \", epoch)\n",
    "        batch_index = 0\n",
    "        count=1\n",
    "        with open('/home/ubuntu/ABCD_genotype/ABCD_QCed_EA_pruned_v2.ped.temp4') as f:\n",
    "            while True:\n",
    "                chunk = list(islice(f, batch))\n",
    "\n",
    "                #print('chunk length: ', len(chunk))\n",
    "                if not chunk:\n",
    "                    break\n",
    "                batch_index += 1\n",
    "                if debug and batch_index%100 == 0:\n",
    "                    print('batch:', batch_index)\n",
    "\n",
    "                if batch_index in train_folds:\n",
    "                    all_data = []\n",
    "                    all_label = []\n",
    "                    for line in chunk:\n",
    "                        line_sp = line.split('\\t')\n",
    "                        orig_key = \"\".join(line_sp[0].split('_')[1:])\n",
    "                        if orig_key in combine.index:\n",
    "                            all_label.append(Y.loc[orig_key, y_columns[0]])\n",
    "                            all_data.append([int(i) for i in line_sp[6:] if i])\n",
    "                    if not all_label:\n",
    "                        del all_data\n",
    "                        gc.collect()\n",
    "                        print('Not found data in this batch!')\n",
    "                        continue\n",
    "\n",
    "                    img = torch.FloatTensor(all_data)\n",
    "                    img = img.view(img.size(0), -1)\n",
    "                    img = img/max_abcd\n",
    "                    label = torch.FloatTensor(all_label)\n",
    "                    label = (label - min_label) / (max_label - min_label) #여기서 정규화한 것 때매 나중에 바꿔줘야함\n",
    "                    del all_data, all_label\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    # =================== nn1 forward=====================)\n",
    "                    output, predict_y= model(img)\n",
    "                    predict_y = predict_y.view(predict_y.size(0))\n",
    "\n",
    "                    if binary_output:\n",
    "                        regression_loss = criterion(predict_y, label.cuda().type(torch.long))\n",
    "                    else:\n",
    "                        regression_loss = criterion(predict_y, label.cuda().type(torch.float))\n",
    "                        with torch.no_grad():\n",
    "                            r_squared = r2_score(label.numpy(), predict_y.cpu().numpy())\n",
    "\n",
    "                    loss = reconstruct_criterion(output[0], img[:, :int(in_channel/2)].cuda(0))\n",
    "                    loss += reconstruct_criterion(output[1], img[:, int(in_channel/2):].cuda(1)).cuda(0)\n",
    "                    \n",
    "                    '''\n",
    "                    print('-'*5)\n",
    "                    \n",
    "                    print('reconstruct loss:{:.7f}'.format(loss.item()))\n",
    "                    print('regression loss:{:.7f}'.format(regression_loss.item()))\n",
    "                    print('R2 score:{:.7f}'.format(r_squared))\n",
    "                    '''\n",
    "                    print('count: ', count)\n",
    "                    #print(loss_result)\n",
    "                    \n",
    "                    count += 1\n",
    "\n",
    "                    # =================== backward ====================\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss = loss + regression_loss\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    del img\n",
    "                    gc.collect()\n",
    "\n",
    "                    #if debug:\n",
    "                    #    return model # early return to save time\n",
    "\n",
    "        # =================== log ========================\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print('epoch [{}/{}]'.format(epoch+1,num_epochs))\n",
    "            print('reconstruct loss:{:.7f}'.format(loss.item()))\n",
    "            print('regression loss:{:.7f}'.format(regression_loss.item()))\n",
    "            print('R2 score:{:.7f}'.format(r_squared))\n",
    "            print('-'*10)\n",
    "            loss_result.loc[int(epoch)]=[epoch, loss.item(), regression_loss.item(), r_squared]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_folds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c210ba97fb1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_folds' is not defined"
     ]
    }
   ],
   "source": [
    "model = train_nn(lr, m, batch, train_folds)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(test_fold, model, batch, mode=0):\n",
    "    print('='*50)\n",
    "    # mode 0: use the passed \"model\" to get prediction on \"test_fold\", not used now\n",
    "    # mode 1: use the passed \"model\" to get encoded SNPs and return them, can then be used for other models\n",
    "\n",
    "    print('begin prediction')\n",
    "    assert mode in (0, 1)\n",
    "    y, y_true, all_encode_x = [], [], []\n",
    "    batch_index = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with open('/home/ubuntu/ABCD_genotype/ABCD_QCed_EA_pruned_v2.ped.temp4') as f:\n",
    "            while True:\n",
    "                chunk = list(islice(f, batch))\n",
    "                if not chunk:\n",
    "                    break\n",
    "                    \n",
    "                batch_index += 1\n",
    "                if batch_index in test_fold:\n",
    "                    all_data = []\n",
    "                    all_label = []\n",
    "                    for line in chunk:\n",
    "                        orig_key = \"\".join(line.split('\\t')[0].split('_')[1:])\n",
    "                        if orig_key in combine.index:\n",
    "                            all_label.append(Y.loc[orig_key, y_columns[0]])\n",
    "                            all_data.append([int(i) for i in line.split('\\t')[6:] if i])\n",
    "                    if not all_label:\n",
    "                        del all_data\n",
    "                        gc.collect()\n",
    "                        print('Not found data in this batch!')\n",
    "                        continue\n",
    "\n",
    "                    img = torch.FloatTensor(all_data)\n",
    "                    img = img/max_abcd\n",
    "                    img = img.view(img.size(0), -1)\n",
    "                    del all_data\n",
    "                    gc.collect()\n",
    "\n",
    "                    y_true.extend(all_label)\n",
    "\n",
    "                    if mode==0:\n",
    "                        predict_y = model.encode_and_predict(img).cpu()\n",
    "                        y.extend([ii*(max_label-min_label) + min_label for ii in predict_y.cpu()])\n",
    "                    else:\n",
    "                        encode_x = model.encode(img).cpu()\n",
    "                        all_encode_x.append(torch.cat((encode_x, prs_data), 1))\n",
    "\n",
    "                    del img\n",
    "                    gc.collect()\n",
    "\n",
    "                    #if debug and batch_index > 1: # early return in debug mode\n",
    "                    #    if mode == 0:\n",
    "                    #        return sum(((y[i] - y_true[i])**2 for i in range(len(y)))) / len(y)\n",
    "                    #    else:\n",
    "                    #        return torch.cat(all_encode_x, 0).numpy(), np.array(y_true)\n",
    "\n",
    "    \n",
    "    if mode==0:\n",
    "        return torch.cat(y, 0).numpy(), np.array(y_true)\n",
    "    else:\n",
    "        return torch.cat(all_encode_x, 0).numpy(), np.array(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_criterion = nn.MSELoss()\n",
    "if binary_output:\n",
    "    #print('output is binary')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    #print('output is continuous')\n",
    "    criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = ['nihtbx_totalcomp_uncorrected'] # label dimension is 1\n",
    "y_dim = len(y_columns)\n",
    "Y = pd.DataFrame(combine, columns = y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Use Only SNPs -----------------\n",
      "batch size: 50 epochs: 100 ae middle dim: 64\n",
      "classify middles: [16] lr: 0.01\n",
      "test fold: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "--------------------------------------------------\n",
      "Training........\n",
      "epoch:  0\n",
      "count:  1\n",
      "count:  2\n",
      "count:  3\n",
      "count:  4\n",
      "count:  5\n",
      "count:  6\n",
      "count:  7\n",
      "count:  8\n",
      "count:  9\n",
      "count:  10\n",
      "count:  11\n",
      "count:  12\n",
      "count:  13\n",
      "count:  14\n",
      "count:  15\n",
      "count:  16\n",
      "count:  17\n",
      "count:  18\n",
      "count:  19\n",
      "count:  20\n",
      "count:  21\n",
      "count:  22\n",
      "count:  23\n",
      "count:  24\n",
      "count:  25\n",
      "count:  26\n",
      "count:  27\n",
      "count:  28\n",
      "count:  29\n",
      "count:  30\n",
      "count:  31\n",
      "count:  32\n",
      "count:  33\n",
      "count:  34\n",
      "count:  35\n",
      "count:  36\n",
      "count:  37\n",
      "count:  38\n",
      "count:  39\n",
      "count:  40\n",
      "count:  41\n",
      "count:  42\n",
      "count:  43\n",
      "count:  44\n",
      "count:  45\n",
      "count:  46\n",
      "count:  47\n",
      "count:  48\n",
      "count:  49\n",
      "count:  50\n",
      "count:  51\n",
      "count:  52\n",
      "count:  53\n",
      "count:  54\n",
      "count:  55\n",
      "count:  56\n",
      "count:  57\n",
      "count:  58\n",
      "count:  59\n",
      "count:  60\n",
      "count:  61\n",
      "count:  62\n",
      "count:  63\n",
      "count:  64\n",
      "count:  65\n",
      "count:  66\n",
      "count:  67\n",
      "count:  68\n",
      "count:  69\n",
      "count:  70\n",
      "count:  71\n",
      "count:  72\n",
      "epoch [1/100]\n",
      "\n",
      "reconstruct loss:0.0384878\n",
      "regression loss:0.1469491\n",
      "R2 score:-16.4082554\n",
      "epoch:  1\n",
      "count:  1\n",
      "count:  2\n",
      "count:  3\n",
      "count:  4\n",
      "count:  5\n",
      "count:  6\n",
      "count:  7\n",
      "count:  8\n",
      "count:  9\n",
      "count:  10\n",
      "count:  11\n",
      "count:  12\n",
      "count:  13\n",
      "count:  14\n",
      "count:  15\n",
      "count:  16\n",
      "count:  17\n",
      "count:  18\n",
      "count:  19\n",
      "count:  20\n",
      "count:  21\n",
      "count:  22\n",
      "count:  23\n",
      "count:  24\n",
      "count:  25\n",
      "count:  26\n",
      "count:  27\n",
      "count:  28\n",
      "count:  29\n",
      "count:  30\n",
      "count:  31\n",
      "count:  32\n",
      "count:  33\n",
      "count:  34\n",
      "count:  35\n",
      "count:  36\n",
      "count:  37\n",
      "count:  38\n",
      "count:  39\n",
      "count:  40\n",
      "count:  41\n",
      "count:  42\n",
      "count:  43\n",
      "count:  44\n",
      "count:  45\n",
      "count:  46\n",
      "count:  47\n",
      "count:  48\n",
      "count:  49\n",
      "count:  50\n",
      "count:  51\n",
      "count:  52\n",
      "count:  53\n",
      "count:  54\n",
      "count:  55\n",
      "count:  56\n",
      "count:  57\n",
      "count:  58\n",
      "count:  59\n",
      "count:  60\n",
      "count:  61\n",
      "count:  62\n",
      "count:  63\n",
      "count:  64\n",
      "count:  65\n",
      "count:  66\n",
      "count:  67\n",
      "count:  68\n",
      "count:  69\n",
      "count:  70\n",
      "count:  71\n",
      "count:  72\n",
      "epoch [2/100]\n",
      "\n",
      "reconstruct loss:0.0384976\n",
      "regression loss:0.0165127\n",
      "R2 score:-0.9561650\n",
      "epoch:  2\n",
      "count:  1\n",
      "count:  2\n",
      "count:  3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0d60dfc6c859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtrain_folds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_folds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NIH_save_epoch100_0730.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2b94ed4d48ea>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(learning_rate, m, batch, train_folds)\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0morig_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                             \u001b[0mall_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                             \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_sp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2b94ed4d48ea>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0morig_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                             \u001b[0mall_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_columns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                             \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_sp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0;32mdel\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('----------------- Use Only SNPs -----------------')\n",
    "print('batch size:', batch, 'epochs:', num_epochs, 'ae middle dim:', m)\n",
    "print('classify middles:', classify_middle, 'lr:', lr)\n",
    "\n",
    "all_folds = []\n",
    "for i in range(k_fold):\n",
    "    batches_per_fold = each_fold_size // batch\n",
    "    all_folds.append(list(range(i*batches_per_fold, (i+1)*batches_per_fold)))\n",
    "\n",
    "for i in range(len(all_folds)):\n",
    "    if i==1: #일단 한 번만 test 하기 위함. 나중에 뺄 것.\n",
    "        break\n",
    "        \n",
    "    print('test fold:', all_folds[i])\n",
    "    train_folds = []\n",
    "    for j in range(len(all_folds)):\n",
    "        if j != i:\n",
    "            train_folds.extend(all_folds[j])\n",
    "\n",
    "    model = train_nn(lr, m, batch, train_folds)\n",
    "    torch.save(model.state_dict(), 'NIH_save_epoch100_0730.pth')\n",
    "    \n",
    "    print(loss_result)\n",
    "    \n",
    "    \n",
    "    plt.plot(loss_result['epoch'], loss_result['regression loss'])\n",
    "    plt.plot(loss_result['epoch'], loss_result['reconstuct loss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(loss_result['epoch'], loss_result['r squared'])\n",
    "    plt.show()\n",
    "    \n",
    "    if not use_rf:\n",
    "        y_pred, y_true = make_prediction(list(range(num_samples)), model, batch, 0)\n",
    "        \n",
    "        assert len(y_pred) == len(y_true)\n",
    "        print('total number of samples: ', len(y_pred))\n",
    "        \n",
    "        print('Test MSE: ', criterion(torch.tensor(y_pred), torch.tensor(y_true)).item())\n",
    "        print('R squared score: ', r2_score(y_true, y_pred))\n",
    "    \n",
    "    else:\n",
    "        # prediction using encoded SNPs and prs with RF model\n",
    "        rf_x, rf_y = make_prediction(list(range(num_samples)), model, batch, 1)\n",
    "        assert rf_x.shape[0] == rf_y.shape[0]\n",
    "        print('total number of samples: ', rf_x.shape[0])\n",
    "\n",
    "        if binary_output:\n",
    "            rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=2)\n",
    "        else:\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=2)\n",
    "\n",
    "        test_index = list(range(all_folds[i][0]*batch, min((1+all_folds[i][-1])*batch, num_samples)))\n",
    "        train_index = [i for i in range(num_samples) if i not in test_index]\n",
    "\n",
    "        rf_model.fit(rf_x[train_index], rf_y[train_index])\n",
    "        if binary_output:\n",
    "            print('Train res: ', criterion(torch.tensor(rf_model.predict_proba(rf_x[train_index])), torch.tensor(rf_y[train_index])).item())\n",
    "            print('Test res: ', criterion(torch.tensor(rf_model.predict_proba(rf_x[test_index])), torch.tensor(rf_y[test_index])).item())\n",
    "        else:\n",
    "            y_test_true = rf_y[test_index]\n",
    "            y_test_prediction = rf_model.predict(rf_x[test_index])\n",
    "            print('Train res: ', criterion(torch.tensor(rf_model.predict(rf_x[train_index])), torch.tensor(rf_y[train_index])).item())\n",
    "            print('Test res: ', criterion(torch.tensor(y_test_prediction), torch.tensor(y_test_true)).item())\n",
    "            print('R squared score: ', r2_score(y_test_true, y_test_prediction))\n",
    "            \n",
    "    # prediction using ae regression network, not used now\n",
    "    #print(\"train res: \", make_prediction(train_folds, model, batch))\n",
    "    #print(\"res: \", make_prediction(all_folds[i], model, batch))\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>reconstuct loss</th>\n",
       "      <th>regression loss</th>\n",
       "      <th>r squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038488</td>\n",
       "      <td>0.146949</td>\n",
       "      <td>-16.408255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>0.016513</td>\n",
       "      <td>-0.956165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  reconstuct loss  regression loss  r squared\n",
       "0    0.0         0.038488         0.146949 -16.408255\n",
       "1    1.0         0.038498         0.016513  -0.956165"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
