{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Parameters:</strong>\n",
    "1. Cross Validation nubmer: 5 (both inner and outer)\n",
    "2. n_estimator: 10, 20, 30, ... , 190, 200\n",
    "3. max_depth: 1, 2, 3, 4, 5, None\n",
    "4. X <- GPS and age, female, hign.educ....\n",
    "\n",
    "<strong>Result:</strong>\n",
    "- best max depth: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('/home/ubuntu/phenotype_and_GPS.csv', header=0)\n",
    "my_data['KEY'] = my_data['KEY'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I placed NAN to mean value... should I replace it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.fillna(my_data.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KEY</th>\n",
       "      <th>AD</th>\n",
       "      <th>DEPRESSION_SUB</th>\n",
       "      <th>IQ</th>\n",
       "      <th>NEUROTICISM</th>\n",
       "      <th>WORRY_SUB</th>\n",
       "      <th>INSOMNIA</th>\n",
       "      <th>PTSD</th>\n",
       "      <th>SNORING</th>\n",
       "      <th>CP</th>\n",
       "      <th>...</th>\n",
       "      <th>income</th>\n",
       "      <th>married</th>\n",
       "      <th>abcd_site</th>\n",
       "      <th>vol</th>\n",
       "      <th>BMI</th>\n",
       "      <th>NIH_totcomp</th>\n",
       "      <th>NIH_flucomp</th>\n",
       "      <th>CBCL_TotProb</th>\n",
       "      <th>CBCL_Internal</th>\n",
       "      <th>CBCL_External</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDARINV003RTV85</td>\n",
       "      <td>-5.780160</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-2.917600</td>\n",
       "      <td>0.013433</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>-0.307129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063292</td>\n",
       "      <td>-2.156561</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.050932</td>\n",
       "      <td>20.480539</td>\n",
       "      <td>0.632147</td>\n",
       "      <td>0.605564</td>\n",
       "      <td>-0.789382</td>\n",
       "      <td>-0.731957</td>\n",
       "      <td>-0.589077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARINV007W6H7B</td>\n",
       "      <td>9.133668</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.369931</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>0.438520</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>-3.683131</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>1.293196</td>\n",
       "      <td>18.234286</td>\n",
       "      <td>1.069812</td>\n",
       "      <td>0.887103</td>\n",
       "      <td>0.268149</td>\n",
       "      <td>0.534371</td>\n",
       "      <td>-0.418541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARINV00BD7VDC</td>\n",
       "      <td>2.059180</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>-4.261953</td>\n",
       "      <td>0.012193</td>\n",
       "      <td>-0.016278</td>\n",
       "      <td>0.071778</td>\n",
       "      <td>0.010899</td>\n",
       "      <td>0.037388</td>\n",
       "      <td>-4.160142</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1.129021</td>\n",
       "      <td>16.329800</td>\n",
       "      <td>0.632147</td>\n",
       "      <td>1.544026</td>\n",
       "      <td>-0.065808</td>\n",
       "      <td>-0.008341</td>\n",
       "      <td>-0.248005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARINV00J52GPG</td>\n",
       "      <td>8.591499</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>-2.582771</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.328521</td>\n",
       "      <td>0.019801</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>-4.682147</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.522359</td>\n",
       "      <td>17</td>\n",
       "      <td>0.020277</td>\n",
       "      <td>19.061896</td>\n",
       "      <td>0.329044</td>\n",
       "      <td>0.237332</td>\n",
       "      <td>-0.399765</td>\n",
       "      <td>-0.912861</td>\n",
       "      <td>-0.589077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARINV00NPMHND</td>\n",
       "      <td>3.104598</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>-3.145087</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008245</td>\n",
       "      <td>-4.843486</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>0.663349</td>\n",
       "      <td>17.663472</td>\n",
       "      <td>-0.680849</td>\n",
       "      <td>-1.365205</td>\n",
       "      <td>1.325680</td>\n",
       "      <td>3.247931</td>\n",
       "      <td>-0.248005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               KEY        AD  DEPRESSION_SUB        IQ  NEUROTICISM  \\\n",
       "0  NDARINV003RTV85 -5.780160        0.002583 -2.917600     0.013433   \n",
       "1  NDARINV007W6H7B  9.133668        0.002060  0.369931     0.000064   \n",
       "2  NDARINV00BD7VDC  2.059180        0.006631 -4.261953     0.012193   \n",
       "3  NDARINV00J52GPG  8.591499        0.004259 -2.582771     0.013623   \n",
       "4  NDARINV00NPMHND  3.104598        0.000878 -3.145087     0.013695   \n",
       "\n",
       "   WORRY_SUB  INSOMNIA      PTSD   SNORING        CP  ...  income   married  \\\n",
       "0  -0.000821 -0.307129  0.000000  0.063292 -2.156561  ...     8.0  1.000000   \n",
       "1  -0.001469  0.438520  0.017525  0.021098 -3.683131  ...    10.0  1.000000   \n",
       "2  -0.016278  0.071778  0.010899  0.037388 -4.160142  ...    10.0  1.000000   \n",
       "3   0.000487 -0.328521  0.019801  0.022991 -4.682147  ...     6.0  1.522359   \n",
       "4  -0.013129  0.014641  0.000000 -0.008245 -4.843486  ...     8.0  1.000000   \n",
       "\n",
       "   abcd_site       vol        BMI  NIH_totcomp  NIH_flucomp  CBCL_TotProb  \\\n",
       "0          6 -1.050932  20.480539     0.632147     0.605564     -0.789382   \n",
       "1         22  1.293196  18.234286     1.069812     0.887103      0.268149   \n",
       "2          7  1.129021  16.329800     0.632147     1.544026     -0.065808   \n",
       "3         17  0.020277  19.061896     0.329044     0.237332     -0.399765   \n",
       "4         17  0.663349  17.663472    -0.680849    -1.365205      1.325680   \n",
       "\n",
       "   CBCL_Internal  CBCL_External  \n",
       "0      -0.731957      -0.589077  \n",
       "1       0.534371      -0.418541  \n",
       "2      -0.008341      -0.248005  \n",
       "3      -0.912861      -0.589077  \n",
       "4       3.247931      -0.248005  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples:  4567\n",
      "fold_size:  914\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5\n",
    "\n",
    "num_samples = len(my_data)\n",
    "each_fold_size = int(num_samples/k_fold) +1\n",
    "\n",
    "print('num_samples: ', num_samples)\n",
    "print('fold_size: ', each_fold_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARAMETERS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AD</th>\n",
       "      <th>DEPRESSION_SUB</th>\n",
       "      <th>IQ</th>\n",
       "      <th>NEUROTICISM</th>\n",
       "      <th>WORRY_SUB</th>\n",
       "      <th>INSOMNIA</th>\n",
       "      <th>PTSD</th>\n",
       "      <th>SNORING</th>\n",
       "      <th>CP</th>\n",
       "      <th>excl23andMe</th>\n",
       "      <th>...</th>\n",
       "      <th>HAPPINESS</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>race</th>\n",
       "      <th>high.educ</th>\n",
       "      <th>income</th>\n",
       "      <th>married</th>\n",
       "      <th>abcd_site</th>\n",
       "      <th>vol</th>\n",
       "      <th>BMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.780160</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>-2.917600</td>\n",
       "      <td>0.013433</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>-0.307129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063292</td>\n",
       "      <td>-2.156561</td>\n",
       "      <td>-0.248034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.050932</td>\n",
       "      <td>20.480539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.133668</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.369931</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>0.438520</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>0.021098</td>\n",
       "      <td>-3.683131</td>\n",
       "      <td>-1.858655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22</td>\n",
       "      <td>1.293196</td>\n",
       "      <td>18.234286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.059180</td>\n",
       "      <td>0.006631</td>\n",
       "      <td>-4.261953</td>\n",
       "      <td>0.012193</td>\n",
       "      <td>-0.016278</td>\n",
       "      <td>0.071778</td>\n",
       "      <td>0.010899</td>\n",
       "      <td>0.037388</td>\n",
       "      <td>-4.160142</td>\n",
       "      <td>-2.382114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001354</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1.129021</td>\n",
       "      <td>16.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.591499</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>-2.582771</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>-0.328521</td>\n",
       "      <td>0.019801</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>-4.682147</td>\n",
       "      <td>-1.212850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.522359</td>\n",
       "      <td>17</td>\n",
       "      <td>0.020277</td>\n",
       "      <td>19.061896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.104598</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>-3.145087</td>\n",
       "      <td>0.013695</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008245</td>\n",
       "      <td>-4.843486</td>\n",
       "      <td>-1.088557</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>0.663349</td>\n",
       "      <td>17.663472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AD  DEPRESSION_SUB        IQ  NEUROTICISM  WORRY_SUB  INSOMNIA  \\\n",
       "0 -5.780160        0.002583 -2.917600     0.013433  -0.000821 -0.307129   \n",
       "1  9.133668        0.002060  0.369931     0.000064  -0.001469  0.438520   \n",
       "2  2.059180        0.006631 -4.261953     0.012193  -0.016278  0.071778   \n",
       "3  8.591499        0.004259 -2.582771     0.013623   0.000487 -0.328521   \n",
       "4  3.104598        0.000878 -3.145087     0.013695  -0.013129  0.014641   \n",
       "\n",
       "       PTSD   SNORING        CP  excl23andMe  ...  HAPPINESS  age  female  \\\n",
       "0  0.000000  0.063292 -2.156561    -0.248034  ...   0.002318  131       1   \n",
       "1  0.017525  0.021098 -3.683131    -1.858655  ...  -0.000128  126       0   \n",
       "2  0.010899  0.037388 -4.160142    -2.382114  ...  -0.001354  112       0   \n",
       "3  0.019801  0.022991 -4.682147    -1.212850  ...   0.001577  110       0   \n",
       "4  0.000000 -0.008245 -4.843486    -1.088557  ...  -0.000256  118       1   \n",
       "\n",
       "   race  high.educ  income   married  abcd_site       vol        BMI  \n",
       "0   1.0       13.0     8.0  1.000000          6 -1.050932  20.480539  \n",
       "1   1.0       19.0    10.0  1.000000         22  1.293196  18.234286  \n",
       "2   1.0       20.0    10.0  1.000000          7  1.129021  16.329800  \n",
       "3   5.0       21.0     6.0  1.522359         17  0.020277  19.061896  \n",
       "4   1.0       16.0     8.0  1.000000         17  0.663349  17.663472  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = my_data.iloc[:, 1:35]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.632147\n",
       "1    1.069812\n",
       "2    0.632147\n",
       "3    0.329044\n",
       "4   -0.680849\n",
       "Name: NIH_totcomp, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = my_data['NIH_totcomp']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X.to_numpy()\n",
    "y = y.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv = KFold(n_splits = k_fold, shuffle=False, random_state = 123)\n",
    "outer_cv = KFold(n_splits = k_fold, shuffle=False, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Loop:  1\n",
      "     mean_test_score  std_test_score                                 params\n",
      "0           0.180836        0.028675   {'max_depth': 4, 'n_estimators': 10}\n",
      "1           0.186887        0.021941   {'max_depth': 4, 'n_estimators': 20}\n",
      "2           0.188975        0.022526   {'max_depth': 4, 'n_estimators': 30}\n",
      "3           0.189256        0.021883   {'max_depth': 4, 'n_estimators': 40}\n",
      "4           0.188608        0.022995   {'max_depth': 4, 'n_estimators': 50}\n",
      "5           0.189391        0.021969   {'max_depth': 4, 'n_estimators': 60}\n",
      "6           0.189081        0.022591   {'max_depth': 4, 'n_estimators': 70}\n",
      "7           0.189850        0.022028   {'max_depth': 4, 'n_estimators': 80}\n",
      "8           0.190053        0.022288   {'max_depth': 4, 'n_estimators': 90}\n",
      "9           0.190059        0.021672  {'max_depth': 4, 'n_estimators': 100}\n",
      "10          0.189835        0.022393  {'max_depth': 4, 'n_estimators': 110}\n",
      "11          0.189589        0.022096  {'max_depth': 4, 'n_estimators': 120}\n",
      "12          0.189554        0.022788  {'max_depth': 4, 'n_estimators': 130}\n",
      "13          0.190231        0.022805  {'max_depth': 4, 'n_estimators': 140}\n",
      "14          0.190746        0.022854  {'max_depth': 4, 'n_estimators': 150}\n",
      "15          0.191610        0.023061  {'max_depth': 4, 'n_estimators': 160}\n",
      "16          0.191226        0.023070  {'max_depth': 4, 'n_estimators': 170}\n",
      "17          0.191292        0.022262  {'max_depth': 4, 'n_estimators': 180}\n",
      "18          0.191215        0.022228  {'max_depth': 4, 'n_estimators': 190}\n",
      "19          0.177922        0.031826   {'max_depth': 5, 'n_estimators': 10}\n",
      "20          0.187369        0.025216   {'max_depth': 5, 'n_estimators': 20}\n",
      "21          0.188183        0.023143   {'max_depth': 5, 'n_estimators': 30}\n",
      "22          0.189144        0.021972   {'max_depth': 5, 'n_estimators': 40}\n",
      "23          0.190945        0.023023   {'max_depth': 5, 'n_estimators': 50}\n",
      "24          0.191868        0.023219   {'max_depth': 5, 'n_estimators': 60}\n",
      "25          0.193053        0.021042   {'max_depth': 5, 'n_estimators': 70}\n",
      "26          0.194004        0.021580   {'max_depth': 5, 'n_estimators': 80}\n",
      "27          0.194084        0.022441   {'max_depth': 5, 'n_estimators': 90}\n",
      "28          0.195024        0.021683  {'max_depth': 5, 'n_estimators': 100}\n",
      "29          0.194208        0.022311  {'max_depth': 5, 'n_estimators': 110}\n",
      "..               ...             ...                                    ...\n",
      "84          0.185548        0.025497   {'max_depth': 8, 'n_estimators': 90}\n",
      "85          0.186876        0.023156  {'max_depth': 8, 'n_estimators': 100}\n",
      "86          0.187672        0.026185  {'max_depth': 8, 'n_estimators': 110}\n",
      "87          0.187065        0.024997  {'max_depth': 8, 'n_estimators': 120}\n",
      "88          0.187470        0.025620  {'max_depth': 8, 'n_estimators': 130}\n",
      "89          0.189039        0.027615  {'max_depth': 8, 'n_estimators': 140}\n",
      "90          0.188111        0.027294  {'max_depth': 8, 'n_estimators': 150}\n",
      "91          0.187407        0.027095  {'max_depth': 8, 'n_estimators': 160}\n",
      "92          0.188415        0.026611  {'max_depth': 8, 'n_estimators': 170}\n",
      "93          0.189487        0.026608  {'max_depth': 8, 'n_estimators': 180}\n",
      "94          0.188852        0.026694  {'max_depth': 8, 'n_estimators': 190}\n",
      "95          0.149961        0.035782   {'max_depth': 9, 'n_estimators': 10}\n",
      "96          0.176500        0.021172   {'max_depth': 9, 'n_estimators': 20}\n",
      "97          0.174888        0.023696   {'max_depth': 9, 'n_estimators': 30}\n",
      "98          0.180048        0.024742   {'max_depth': 9, 'n_estimators': 40}\n",
      "99          0.182392        0.022053   {'max_depth': 9, 'n_estimators': 50}\n",
      "100         0.180592        0.027372   {'max_depth': 9, 'n_estimators': 60}\n",
      "101         0.187935        0.023502   {'max_depth': 9, 'n_estimators': 70}\n",
      "102         0.184382        0.026172   {'max_depth': 9, 'n_estimators': 80}\n",
      "103         0.185972        0.024989   {'max_depth': 9, 'n_estimators': 90}\n",
      "104         0.188541        0.024448  {'max_depth': 9, 'n_estimators': 100}\n",
      "105         0.189532        0.022510  {'max_depth': 9, 'n_estimators': 110}\n",
      "106         0.190266        0.023712  {'max_depth': 9, 'n_estimators': 120}\n",
      "107         0.190445        0.023331  {'max_depth': 9, 'n_estimators': 130}\n",
      "108         0.190137        0.023504  {'max_depth': 9, 'n_estimators': 140}\n",
      "109         0.190878        0.023254  {'max_depth': 9, 'n_estimators': 150}\n",
      "110         0.189582        0.023461  {'max_depth': 9, 'n_estimators': 160}\n",
      "111         0.189214        0.021970  {'max_depth': 9, 'n_estimators': 170}\n",
      "112         0.188430        0.022049  {'max_depth': 9, 'n_estimators': 180}\n",
      "113         0.191145        0.022888  {'max_depth': 9, 'n_estimators': 190}\n",
      "\n",
      "[114 rows x 3 columns]\n",
      "\n",
      "\n",
      "best train parameter:  {'max_depth': 5, 'n_estimators': 100}\n",
      "best train score:  0.19502408146986308\n",
      "\n",
      "\n",
      "test score:  0.13494451424129095\n",
      "----------------------------\n",
      "----------------------------\n",
      "Loop:  2\n",
      "     mean_test_score  std_test_score                                 params\n",
      "0           0.160802        0.031982   {'max_depth': 4, 'n_estimators': 10}\n",
      "1           0.170140        0.030894   {'max_depth': 4, 'n_estimators': 20}\n",
      "2           0.171128        0.031509   {'max_depth': 4, 'n_estimators': 30}\n",
      "3           0.171338        0.030207   {'max_depth': 4, 'n_estimators': 40}\n",
      "4           0.172770        0.030261   {'max_depth': 4, 'n_estimators': 50}\n",
      "5           0.173307        0.030281   {'max_depth': 4, 'n_estimators': 60}\n",
      "6           0.174008        0.030776   {'max_depth': 4, 'n_estimators': 70}\n",
      "7           0.174224        0.029983   {'max_depth': 4, 'n_estimators': 80}\n",
      "8           0.174453        0.030486   {'max_depth': 4, 'n_estimators': 90}\n",
      "9           0.175079        0.030199  {'max_depth': 4, 'n_estimators': 100}\n",
      "10          0.174314        0.029642  {'max_depth': 4, 'n_estimators': 110}\n",
      "11          0.175122        0.030068  {'max_depth': 4, 'n_estimators': 120}\n",
      "12          0.174677        0.028908  {'max_depth': 4, 'n_estimators': 130}\n",
      "13          0.175446        0.028408  {'max_depth': 4, 'n_estimators': 140}\n",
      "14          0.175023        0.028565  {'max_depth': 4, 'n_estimators': 150}\n",
      "15          0.174688        0.028254  {'max_depth': 4, 'n_estimators': 160}\n",
      "16          0.174536        0.029208  {'max_depth': 4, 'n_estimators': 170}\n",
      "17          0.173787        0.028841  {'max_depth': 4, 'n_estimators': 180}\n",
      "18          0.173694        0.028977  {'max_depth': 4, 'n_estimators': 190}\n",
      "19          0.166012        0.022474   {'max_depth': 5, 'n_estimators': 10}\n",
      "20          0.173578        0.030994   {'max_depth': 5, 'n_estimators': 20}\n",
      "21          0.171665        0.027226   {'max_depth': 5, 'n_estimators': 30}\n",
      "22          0.172649        0.027743   {'max_depth': 5, 'n_estimators': 40}\n",
      "23          0.177346        0.028629   {'max_depth': 5, 'n_estimators': 50}\n",
      "24          0.176205        0.027394   {'max_depth': 5, 'n_estimators': 60}\n",
      "25          0.177371        0.028200   {'max_depth': 5, 'n_estimators': 70}\n",
      "26          0.176440        0.028873   {'max_depth': 5, 'n_estimators': 80}\n",
      "27          0.177816        0.028465   {'max_depth': 5, 'n_estimators': 90}\n",
      "28          0.178182        0.027983  {'max_depth': 5, 'n_estimators': 100}\n",
      "29          0.177876        0.028731  {'max_depth': 5, 'n_estimators': 110}\n",
      "..               ...             ...                                    ...\n",
      "84          0.176584        0.028245   {'max_depth': 8, 'n_estimators': 90}\n",
      "85          0.177673        0.026858  {'max_depth': 8, 'n_estimators': 100}\n",
      "86          0.177304        0.025360  {'max_depth': 8, 'n_estimators': 110}\n",
      "87          0.178429        0.026043  {'max_depth': 8, 'n_estimators': 120}\n",
      "88          0.175842        0.024653  {'max_depth': 8, 'n_estimators': 130}\n",
      "89          0.176579        0.026219  {'max_depth': 8, 'n_estimators': 140}\n",
      "90          0.176568        0.024302  {'max_depth': 8, 'n_estimators': 150}\n",
      "91          0.176481        0.025506  {'max_depth': 8, 'n_estimators': 160}\n",
      "92          0.177217        0.024557  {'max_depth': 8, 'n_estimators': 170}\n",
      "93          0.178281        0.025641  {'max_depth': 8, 'n_estimators': 180}\n",
      "94          0.178549        0.025375  {'max_depth': 8, 'n_estimators': 190}\n",
      "95          0.135934        0.035808   {'max_depth': 9, 'n_estimators': 10}\n",
      "96          0.150875        0.024320   {'max_depth': 9, 'n_estimators': 20}\n",
      "97          0.166763        0.024130   {'max_depth': 9, 'n_estimators': 30}\n",
      "98          0.174308        0.027995   {'max_depth': 9, 'n_estimators': 40}\n",
      "99          0.172003        0.024472   {'max_depth': 9, 'n_estimators': 50}\n",
      "100         0.170737        0.027238   {'max_depth': 9, 'n_estimators': 60}\n",
      "101         0.171441        0.025273   {'max_depth': 9, 'n_estimators': 70}\n",
      "102         0.173661        0.028764   {'max_depth': 9, 'n_estimators': 80}\n",
      "103         0.175179        0.025097   {'max_depth': 9, 'n_estimators': 90}\n",
      "104         0.176435        0.023558  {'max_depth': 9, 'n_estimators': 100}\n",
      "105         0.176565        0.024538  {'max_depth': 9, 'n_estimators': 110}\n",
      "106         0.175221        0.026940  {'max_depth': 9, 'n_estimators': 120}\n",
      "107         0.179395        0.023793  {'max_depth': 9, 'n_estimators': 130}\n",
      "108         0.179739        0.024864  {'max_depth': 9, 'n_estimators': 140}\n",
      "109         0.178727        0.025508  {'max_depth': 9, 'n_estimators': 150}\n",
      "110         0.178682        0.026332  {'max_depth': 9, 'n_estimators': 160}\n",
      "111         0.178315        0.023693  {'max_depth': 9, 'n_estimators': 170}\n",
      "112         0.177259        0.023949  {'max_depth': 9, 'n_estimators': 180}\n",
      "113         0.179743        0.026366  {'max_depth': 9, 'n_estimators': 190}\n",
      "\n",
      "[114 rows x 3 columns]\n",
      "\n",
      "\n",
      "best train parameter:  {'max_depth': 6, 'n_estimators': 170}\n",
      "best train score:  0.18028155869623197\n",
      "\n",
      "\n",
      "test score:  0.1897231340309583\n",
      "----------------------------\n",
      "----------------------------\n",
      "Loop:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     mean_test_score  std_test_score                                 params\n",
      "0           0.172796        0.021562   {'max_depth': 4, 'n_estimators': 10}\n",
      "1           0.174815        0.021285   {'max_depth': 4, 'n_estimators': 20}\n",
      "2           0.175454        0.023143   {'max_depth': 4, 'n_estimators': 30}\n",
      "3           0.176798        0.024512   {'max_depth': 4, 'n_estimators': 40}\n",
      "4           0.177509        0.025217   {'max_depth': 4, 'n_estimators': 50}\n",
      "5           0.178280        0.027518   {'max_depth': 4, 'n_estimators': 60}\n",
      "6           0.179817        0.028535   {'max_depth': 4, 'n_estimators': 70}\n",
      "7           0.179721        0.029310   {'max_depth': 4, 'n_estimators': 80}\n",
      "8           0.180255        0.028504   {'max_depth': 4, 'n_estimators': 90}\n",
      "9           0.180732        0.027312  {'max_depth': 4, 'n_estimators': 100}\n",
      "10          0.181364        0.026485  {'max_depth': 4, 'n_estimators': 110}\n",
      "11          0.181523        0.025740  {'max_depth': 4, 'n_estimators': 120}\n",
      "12          0.181482        0.025375  {'max_depth': 4, 'n_estimators': 130}\n",
      "13          0.181340        0.025201  {'max_depth': 4, 'n_estimators': 140}\n",
      "14          0.181208        0.026416  {'max_depth': 4, 'n_estimators': 150}\n",
      "15          0.180317        0.026625  {'max_depth': 4, 'n_estimators': 160}\n",
      "16          0.180392        0.026534  {'max_depth': 4, 'n_estimators': 170}\n",
      "17          0.180933        0.026620  {'max_depth': 4, 'n_estimators': 180}\n",
      "18          0.181497        0.027010  {'max_depth': 4, 'n_estimators': 190}\n",
      "19          0.174072        0.031194   {'max_depth': 5, 'n_estimators': 10}\n",
      "20          0.176689        0.027568   {'max_depth': 5, 'n_estimators': 20}\n",
      "21          0.179184        0.028115   {'max_depth': 5, 'n_estimators': 30}\n",
      "22          0.179320        0.024946   {'max_depth': 5, 'n_estimators': 40}\n",
      "23          0.181845        0.026076   {'max_depth': 5, 'n_estimators': 50}\n",
      "24          0.181440        0.026819   {'max_depth': 5, 'n_estimators': 60}\n",
      "25          0.182739        0.027792   {'max_depth': 5, 'n_estimators': 70}\n",
      "26          0.185102        0.026911   {'max_depth': 5, 'n_estimators': 80}\n",
      "27          0.184220        0.026047   {'max_depth': 5, 'n_estimators': 90}\n",
      "28          0.184538        0.026828  {'max_depth': 5, 'n_estimators': 100}\n",
      "29          0.182426        0.027099  {'max_depth': 5, 'n_estimators': 110}\n",
      "..               ...             ...                                    ...\n",
      "84          0.178190        0.025213   {'max_depth': 8, 'n_estimators': 90}\n",
      "85          0.181235        0.025932  {'max_depth': 8, 'n_estimators': 100}\n",
      "86          0.179101        0.027621  {'max_depth': 8, 'n_estimators': 110}\n",
      "87          0.180670        0.024523  {'max_depth': 8, 'n_estimators': 120}\n",
      "88          0.176880        0.028285  {'max_depth': 8, 'n_estimators': 130}\n",
      "89          0.177757        0.027246  {'max_depth': 8, 'n_estimators': 140}\n",
      "90          0.178048        0.026349  {'max_depth': 8, 'n_estimators': 150}\n",
      "91          0.178802        0.026957  {'max_depth': 8, 'n_estimators': 160}\n",
      "92          0.180091        0.026260  {'max_depth': 8, 'n_estimators': 170}\n",
      "93          0.182704        0.025484  {'max_depth': 8, 'n_estimators': 180}\n",
      "94          0.183093        0.025199  {'max_depth': 8, 'n_estimators': 190}\n",
      "95          0.139750        0.038270   {'max_depth': 9, 'n_estimators': 10}\n",
      "96          0.159681        0.021001   {'max_depth': 9, 'n_estimators': 20}\n",
      "97          0.172213        0.025577   {'max_depth': 9, 'n_estimators': 30}\n",
      "98          0.169255        0.024082   {'max_depth': 9, 'n_estimators': 40}\n",
      "99          0.172042        0.025763   {'max_depth': 9, 'n_estimators': 50}\n",
      "100         0.180647        0.027536   {'max_depth': 9, 'n_estimators': 60}\n",
      "101         0.179202        0.026264   {'max_depth': 9, 'n_estimators': 70}\n",
      "102         0.179494        0.024908   {'max_depth': 9, 'n_estimators': 80}\n",
      "103         0.178588        0.025190   {'max_depth': 9, 'n_estimators': 90}\n",
      "104         0.183060        0.024403  {'max_depth': 9, 'n_estimators': 100}\n",
      "105         0.178952        0.024546  {'max_depth': 9, 'n_estimators': 110}\n",
      "106         0.180678        0.025480  {'max_depth': 9, 'n_estimators': 120}\n",
      "107         0.180596        0.028552  {'max_depth': 9, 'n_estimators': 130}\n",
      "108         0.181887        0.029168  {'max_depth': 9, 'n_estimators': 140}\n",
      "109         0.181182        0.026817  {'max_depth': 9, 'n_estimators': 150}\n",
      "110         0.181682        0.027857  {'max_depth': 9, 'n_estimators': 160}\n",
      "111         0.182613        0.027513  {'max_depth': 9, 'n_estimators': 170}\n",
      "112         0.182836        0.027771  {'max_depth': 9, 'n_estimators': 180}\n",
      "113         0.181973        0.029067  {'max_depth': 9, 'n_estimators': 190}\n",
      "\n",
      "[114 rows x 3 columns]\n",
      "\n",
      "\n",
      "best train parameter:  {'max_depth': 6, 'n_estimators': 80}\n",
      "best train score:  0.18577257557719687\n",
      "\n",
      "\n",
      "test score:  0.1678837216111697\n",
      "----------------------------\n",
      "----------------------------\n",
      "Loop:  4\n",
      "     mean_test_score  std_test_score                                 params\n",
      "0           0.152455        0.019780   {'max_depth': 4, 'n_estimators': 10}\n",
      "1           0.157470        0.024077   {'max_depth': 4, 'n_estimators': 20}\n",
      "2           0.156682        0.021691   {'max_depth': 4, 'n_estimators': 30}\n",
      "3           0.156625        0.021414   {'max_depth': 4, 'n_estimators': 40}\n",
      "4           0.157774        0.022910   {'max_depth': 4, 'n_estimators': 50}\n",
      "5           0.158812        0.021719   {'max_depth': 4, 'n_estimators': 60}\n",
      "6           0.159366        0.019959   {'max_depth': 4, 'n_estimators': 70}\n",
      "7           0.161141        0.019754   {'max_depth': 4, 'n_estimators': 80}\n",
      "8           0.161092        0.019831   {'max_depth': 4, 'n_estimators': 90}\n",
      "9           0.161513        0.019607  {'max_depth': 4, 'n_estimators': 100}\n",
      "10          0.161705        0.020058  {'max_depth': 4, 'n_estimators': 110}\n",
      "11          0.161997        0.020484  {'max_depth': 4, 'n_estimators': 120}\n",
      "12          0.162547        0.020890  {'max_depth': 4, 'n_estimators': 130}\n",
      "13          0.162219        0.020761  {'max_depth': 4, 'n_estimators': 140}\n",
      "14          0.161861        0.021612  {'max_depth': 4, 'n_estimators': 150}\n",
      "15          0.163100        0.020997  {'max_depth': 4, 'n_estimators': 160}\n",
      "16          0.162678        0.020684  {'max_depth': 4, 'n_estimators': 170}\n",
      "17          0.162671        0.020315  {'max_depth': 4, 'n_estimators': 180}\n",
      "18          0.163158        0.019920  {'max_depth': 4, 'n_estimators': 190}\n",
      "19          0.155970        0.022660   {'max_depth': 5, 'n_estimators': 10}\n",
      "20          0.162902        0.013135   {'max_depth': 5, 'n_estimators': 20}\n",
      "21          0.163795        0.020369   {'max_depth': 5, 'n_estimators': 30}\n",
      "22          0.161565        0.021028   {'max_depth': 5, 'n_estimators': 40}\n",
      "23          0.162083        0.019624   {'max_depth': 5, 'n_estimators': 50}\n",
      "24          0.164155        0.017295   {'max_depth': 5, 'n_estimators': 60}\n",
      "25          0.164888        0.019027   {'max_depth': 5, 'n_estimators': 70}\n",
      "26          0.165688        0.019105   {'max_depth': 5, 'n_estimators': 80}\n",
      "27          0.164148        0.019086   {'max_depth': 5, 'n_estimators': 90}\n",
      "28          0.165457        0.020400  {'max_depth': 5, 'n_estimators': 100}\n",
      "29          0.165029        0.021043  {'max_depth': 5, 'n_estimators': 110}\n",
      "..               ...             ...                                    ...\n",
      "84          0.165561        0.018355   {'max_depth': 8, 'n_estimators': 90}\n",
      "85          0.164426        0.017536  {'max_depth': 8, 'n_estimators': 100}\n",
      "86          0.163024        0.018390  {'max_depth': 8, 'n_estimators': 110}\n",
      "87          0.166883        0.016578  {'max_depth': 8, 'n_estimators': 120}\n",
      "88          0.166463        0.017312  {'max_depth': 8, 'n_estimators': 130}\n",
      "89          0.165477        0.017958  {'max_depth': 8, 'n_estimators': 140}\n",
      "90          0.166038        0.015968  {'max_depth': 8, 'n_estimators': 150}\n",
      "91          0.165160        0.017379  {'max_depth': 8, 'n_estimators': 160}\n",
      "92          0.165256        0.017671  {'max_depth': 8, 'n_estimators': 170}\n",
      "93          0.165488        0.018656  {'max_depth': 8, 'n_estimators': 180}\n",
      "94          0.165684        0.017633  {'max_depth': 8, 'n_estimators': 190}\n",
      "95          0.116745        0.026648   {'max_depth': 9, 'n_estimators': 10}\n",
      "96          0.139977        0.016603   {'max_depth': 9, 'n_estimators': 20}\n",
      "97          0.146356        0.019379   {'max_depth': 9, 'n_estimators': 30}\n",
      "98          0.159119        0.022375   {'max_depth': 9, 'n_estimators': 40}\n",
      "99          0.154784        0.017402   {'max_depth': 9, 'n_estimators': 50}\n",
      "100         0.160756        0.017749   {'max_depth': 9, 'n_estimators': 60}\n",
      "101         0.161596        0.015976   {'max_depth': 9, 'n_estimators': 70}\n",
      "102         0.158051        0.021899   {'max_depth': 9, 'n_estimators': 80}\n",
      "103         0.162371        0.019238   {'max_depth': 9, 'n_estimators': 90}\n",
      "104         0.159128        0.017248  {'max_depth': 9, 'n_estimators': 100}\n",
      "105         0.163025        0.014106  {'max_depth': 9, 'n_estimators': 110}\n",
      "106         0.159005        0.020768  {'max_depth': 9, 'n_estimators': 120}\n",
      "107         0.163179        0.019178  {'max_depth': 9, 'n_estimators': 130}\n",
      "108         0.164688        0.016595  {'max_depth': 9, 'n_estimators': 140}\n",
      "109         0.163628        0.018447  {'max_depth': 9, 'n_estimators': 150}\n",
      "110         0.164589        0.018310  {'max_depth': 9, 'n_estimators': 160}\n",
      "111         0.163445        0.020104  {'max_depth': 9, 'n_estimators': 170}\n",
      "112         0.164687        0.020976  {'max_depth': 9, 'n_estimators': 180}\n",
      "113         0.163677        0.021303  {'max_depth': 9, 'n_estimators': 190}\n",
      "\n",
      "[114 rows x 3 columns]\n",
      "\n",
      "\n",
      "best train parameter:  {'max_depth': 7, 'n_estimators': 190}\n",
      "best train score:  0.1680303330431168\n",
      "\n",
      "\n",
      "test score:  0.22914771861449235\n",
      "----------------------------\n",
      "----------------------------\n",
      "Loop:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     mean_test_score  std_test_score                                 params\n",
      "0           0.163975        0.034333   {'max_depth': 4, 'n_estimators': 10}\n",
      "1           0.171273        0.031428   {'max_depth': 4, 'n_estimators': 20}\n",
      "2           0.174410        0.031258   {'max_depth': 4, 'n_estimators': 30}\n",
      "3           0.174077        0.029468   {'max_depth': 4, 'n_estimators': 40}\n",
      "4           0.176204        0.029648   {'max_depth': 4, 'n_estimators': 50}\n",
      "5           0.175839        0.028837   {'max_depth': 4, 'n_estimators': 60}\n",
      "6           0.174297        0.028400   {'max_depth': 4, 'n_estimators': 70}\n",
      "7           0.173328        0.029089   {'max_depth': 4, 'n_estimators': 80}\n",
      "8           0.172706        0.028563   {'max_depth': 4, 'n_estimators': 90}\n",
      "9           0.172518        0.029147  {'max_depth': 4, 'n_estimators': 100}\n",
      "10          0.173339        0.028888  {'max_depth': 4, 'n_estimators': 110}\n",
      "11          0.174661        0.029067  {'max_depth': 4, 'n_estimators': 120}\n",
      "12          0.174738        0.028941  {'max_depth': 4, 'n_estimators': 130}\n",
      "13          0.174938        0.029171  {'max_depth': 4, 'n_estimators': 140}\n",
      "14          0.174469        0.028751  {'max_depth': 4, 'n_estimators': 150}\n",
      "15          0.173725        0.028820  {'max_depth': 4, 'n_estimators': 160}\n",
      "16          0.172544        0.028366  {'max_depth': 4, 'n_estimators': 170}\n",
      "17          0.172802        0.028574  {'max_depth': 4, 'n_estimators': 180}\n",
      "18          0.173340        0.028773  {'max_depth': 4, 'n_estimators': 190}\n",
      "19          0.163841        0.034191   {'max_depth': 5, 'n_estimators': 10}\n",
      "20          0.165512        0.031567   {'max_depth': 5, 'n_estimators': 20}\n",
      "21          0.169585        0.030983   {'max_depth': 5, 'n_estimators': 30}\n",
      "22          0.170652        0.027911   {'max_depth': 5, 'n_estimators': 40}\n",
      "23          0.173873        0.033006   {'max_depth': 5, 'n_estimators': 50}\n",
      "24          0.175259        0.032025   {'max_depth': 5, 'n_estimators': 60}\n",
      "25          0.175731        0.032303   {'max_depth': 5, 'n_estimators': 70}\n",
      "26          0.177683        0.030128   {'max_depth': 5, 'n_estimators': 80}\n",
      "27          0.175895        0.028172   {'max_depth': 5, 'n_estimators': 90}\n",
      "28          0.175117        0.029810  {'max_depth': 5, 'n_estimators': 100}\n",
      "29          0.177151        0.029107  {'max_depth': 5, 'n_estimators': 110}\n",
      "..               ...             ...                                    ...\n",
      "84          0.172734        0.030257   {'max_depth': 8, 'n_estimators': 90}\n",
      "85          0.177426        0.034318  {'max_depth': 8, 'n_estimators': 100}\n",
      "86          0.176418        0.033998  {'max_depth': 8, 'n_estimators': 110}\n",
      "87          0.178237        0.032851  {'max_depth': 8, 'n_estimators': 120}\n",
      "88          0.177457        0.033865  {'max_depth': 8, 'n_estimators': 130}\n",
      "89          0.175751        0.030664  {'max_depth': 8, 'n_estimators': 140}\n",
      "90          0.176951        0.030671  {'max_depth': 8, 'n_estimators': 150}\n",
      "91          0.178020        0.033315  {'max_depth': 8, 'n_estimators': 160}\n",
      "92          0.181494        0.030808  {'max_depth': 8, 'n_estimators': 170}\n",
      "93          0.179382        0.028954  {'max_depth': 8, 'n_estimators': 180}\n",
      "94          0.180675        0.029831  {'max_depth': 8, 'n_estimators': 190}\n",
      "95          0.129635        0.038340   {'max_depth': 9, 'n_estimators': 10}\n",
      "96          0.158653        0.025221   {'max_depth': 9, 'n_estimators': 20}\n",
      "97          0.170771        0.021365   {'max_depth': 9, 'n_estimators': 30}\n",
      "98          0.169079        0.028336   {'max_depth': 9, 'n_estimators': 40}\n",
      "99          0.173405        0.031558   {'max_depth': 9, 'n_estimators': 50}\n",
      "100         0.170013        0.031302   {'max_depth': 9, 'n_estimators': 60}\n",
      "101         0.175714        0.032139   {'max_depth': 9, 'n_estimators': 70}\n",
      "102         0.175149        0.029303   {'max_depth': 9, 'n_estimators': 80}\n",
      "103         0.178892        0.032473   {'max_depth': 9, 'n_estimators': 90}\n",
      "104         0.175708        0.029148  {'max_depth': 9, 'n_estimators': 100}\n",
      "105         0.176617        0.029968  {'max_depth': 9, 'n_estimators': 110}\n",
      "106         0.177715        0.032988  {'max_depth': 9, 'n_estimators': 120}\n",
      "107         0.176292        0.032856  {'max_depth': 9, 'n_estimators': 130}\n",
      "108         0.178918        0.030269  {'max_depth': 9, 'n_estimators': 140}\n",
      "109         0.178297        0.031583  {'max_depth': 9, 'n_estimators': 150}\n",
      "110         0.178099        0.031722  {'max_depth': 9, 'n_estimators': 160}\n",
      "111         0.179557        0.030104  {'max_depth': 9, 'n_estimators': 170}\n",
      "112         0.179944        0.030198  {'max_depth': 9, 'n_estimators': 180}\n",
      "113         0.177327        0.030505  {'max_depth': 9, 'n_estimators': 190}\n",
      "\n",
      "[114 rows x 3 columns]\n",
      "\n",
      "\n",
      "best train parameter:  {'max_depth': 7, 'n_estimators': 60}\n",
      "best train score:  0.18210279539289073\n",
      "\n",
      "\n",
      "test score:  0.1984883480588555\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "outerloop = 0\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X, y):\n",
    "    outerloop += 1\n",
    "    print('----------------------------')\n",
    "    print('Loop: ', outerloop)\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    params = {'n_estimators': np.arange(10, 200, 10),\n",
    "              'max_depth': [4,5,6,7,8,9]}\n",
    "    \n",
    "    rf = RandomForestRegressor()\n",
    "    rf_grid = GridSearchCV(estimator = rf, param_grid = params, cv = inner_cv, scoring='r2', n_jobs=-1, return_train_score=True)\n",
    "    \n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    y_pred_test = rf_grid.predict(X_test)\n",
    "    #y_pred_train = rf_grid.predict(X_train)\n",
    "    \n",
    "    result = pd.DataFrame(rf_grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n",
    "    print(result)\n",
    "    result.to_csv(\"intel_optimize_1.csv\", index=False)\n",
    "    print('\\n')\n",
    "    print('best train parameter: ', rf_grid.best_params_)\n",
    "    print('best train score: ', rf_grid.best_score_)\n",
    "    print('\\n')\n",
    "    print('test score: ', r2_score(y_test, y_pred_test))\n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### score is R2 score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
